"""
Misaka Cipher - Memory API Routes
FastAPI routes for memory management and visualization
"""

from fastapi import APIRouter, HTTPException
from typing import List, Dict, Any, Optional
from pathlib import Path
import json
import asyncio

from core.utils import get_logger
from core.memory import get_episodic_memory, get_knowledge_graph

logger = get_logger("web.memory_routes")

# Create router
router = APIRouter(prefix="/api/memory", tags=["memory"])


@router.get("/overview")
async def get_memory_overview():
    """
    Get comprehensive memory overview.
    Returns Permanent Memory (Core Insights) and Thread Memory.
    """
    try:
        # 1. Fetch Permanent Memory (Core Insights)
        kg = get_knowledge_graph()
        permanent_memory = []
        
        try:
            # Iterate graph nodes to find core insights
            for node, data in kg.graph.nodes(data=True):
                if data.get('node_type') == 'core_insight':
                    permanent_memory.append({
                        "id": node,
                        "summary": data.get('summary', 'No summary'),
                        "created_at": data.get('created', ''),
                        "confidence": data.get('confidence', 0.0),
                        "tags": data.get('tags', [])
                    })
        except Exception as e:
            logger.error(f"Error fetching permanent memory: {e}")
            
        # 2. Fetch Thread Memory
        episodic = get_episodic_memory()
        threads_memory = []
        
        # Path to threads directory in hierarchical workspace structure
        workspace = Path(__file__).parent.parent.parent.parent
        workspaces_dir = workspace / "memory" / "storage" / "workspaces"
        
        if workspaces_dir.exists():
            # Get all thread JSON files inside hierarchical folders
            thread_files = sorted(workspaces_dir.glob("thread-*/thread-*.json"), key=lambda p: p.stat().st_mtime, reverse=True)
            
            for thread_file in thread_files:
                try:
                    with open(thread_file, 'r') as f:
                        thread_data = json.load(f)
                        
                    task_ids = thread_data.get('task_ids', [])
                    thread_memories = []
                    
                    logger.info(f"Loading thread {thread_data.get('id')} with tasks: {task_ids}")
                    
                    # Dynamically get tasks_dir from the thread's own workspace folder
                    tasks_dir = thread_file.parent / "tasks"
                    
                    # Fetch data for each task
                    for task_id in task_ids:
                        task_found = False
                        
                        # 1. Try to load from Task JSON (Primary Source)
                        task_path = tasks_dir / f"{task_id}.json"
                        if task_path.exists():
                            try:
                                with open(task_path, 'r', encoding='utf-8') as tf:
                                    task_data = json.load(tf)
                                    
                                # Create memory entry from Task Data
                                result = task_data.get('result', {}) or {}
                                summary = f"Task: {task_data.get('prompt', '')[:50]}..."
                                if result.get('tools_forged'):
                                    summary = f"Forged: {', '.join(result['tools_forged'])}"
                                elif result.get('agents_spawned'):
                                    summary = f"Spawned: {', '.join(result['agents_spawned'])}"
                                
                                memory_entry = {
                                    "memory_id": task_data.get('id'),
                                    "trace_id": task_data.get('id'),
                                    "event_type": "task_execution",
                                    "summary": summary,
                                    "content": f"Prompt: {task_data.get('prompt')}\n\nResponse: {result.get('response', '')}",
                                    "timestamp": task_data.get('created_at', ''),
                                    "domain": "Task", # Could extract from metadata if available
                                    "details": task_data # Include full JSON as requested
                                }
                                thread_memories.append(memory_entry)
                                task_found = True
                            except Exception as te:
                                logger.error(f"Failed to load task file {task_path}: {te}")

                        # 2. If not found or if we want semantic info, check Episodic Store (Secondary/Fallback)
                        # We query this regardless to see if there are *additional* related memories (like sub-steps),
                        # but we dedup based on trace_id if we already loaded the main task.
                        memories = episodic.get_by_trace_id(task_id)
                        
                        if not task_found and not memories:
                             logger.warning(f"No data found for task {task_id}")
                        
                        for mem in memories:
                            # Avoid duplicates if we already loaded this task from file
                            # (unless it's a distinct memory event for the same trace)
                            if task_found and mem.event_type == 'task_execution': 
                                continue
                                
                            thread_memories.append({
                                "memory_id": mem.memory_id,
                                "trace_id": mem.trace_id,
                                "event_type": mem.event_type,
                                "summary": mem.summary,
                                "content": mem.content,
                                "timestamp": mem.timestamp,
                                "domain": mem.domain
                            })
                            
                    # Sort memories by timestamp
                    thread_memories.sort(key=lambda x: x.get('timestamp', ''), reverse=True)
                    
                    threads_memory.append({
                        "id": thread_data.get('id'),
                        "title": thread_data.get('title', 'Untitled Thread'),
                        "updated_at": thread_data.get('updated_at'),
                        "memory_count": len(thread_memories),
                        "memories": thread_memories
                    })
                    
                except Exception as e:
                    logger.warning(f"Error reading thread file {thread_file}: {e}")
                    continue
        
        return {
            "permanent": permanent_memory,
            "threads": threads_memory
        }

    except Exception as e:
        logger.error(f"Error generating memory overview: {e}")
        raise HTTPException(status_code=500, detail=str(e))
